{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Task 1: Scrape Weather Information\n",
    "def scrape_weather():\n",
    "    with open(\"weather.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    \n",
    "    weather_data = []\n",
    "    rows = soup.find(\"table\").find(\"tbody\").find_all(\"tr\")\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        day, temp, condition = cols[0].text, int(cols[1].text[:-2]), cols[2].text\n",
    "        weather_data.append({\"day\": day, \"temperature\": temp, \"condition\": condition})\n",
    "    \n",
    "    print(\"Weather Forecast:\")\n",
    "    for entry in weather_data:\n",
    "        print(entry)\n",
    "    \n",
    "    highest_temp = max(weather_data, key=lambda x: x[\"temperature\"])\n",
    "    print(\"Hottest Day:\", highest_temp[\"day\"], highest_temp[\"temperature\"], \"°C\")\n",
    "    \n",
    "    sunny_days = [entry[\"day\"] for entry in weather_data if entry[\"condition\"] == \"Sunny\"]\n",
    "    print(\"Sunny Days:\", sunny_days)\n",
    "    \n",
    "    avg_temp = sum(entry[\"temperature\"] for entry in weather_data) / len(weather_data)\n",
    "    print(\"Average Temperature:\", avg_temp, \"°C\")\n",
    "\n",
    "# Task 2: Scrape Job Listings\n",
    "def scrape_jobs():\n",
    "    url = \"https://realpython.github.io/fake-jobs\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    conn = sqlite3.connect(\"jobs.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS jobs (\n",
    "        job_title TEXT,\n",
    "        company_name TEXT,\n",
    "        location TEXT,\n",
    "        job_description TEXT,\n",
    "        application_link TEXT,\n",
    "        UNIQUE(job_title, company_name, location)\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    job_listings = []\n",
    "    jobs = soup.find_all(\"div\", class_=\"card-content\")\n",
    "    \n",
    "    for job in jobs:\n",
    "        title = job.find(\"h2\", class_=\"title\").text.strip()\n",
    "        company = job.find(\"h3\", class_=\"company\").text.strip()\n",
    "        location = job.find(\"p\", class_=\"location\").text.strip()\n",
    "        description = job.find(\"div\", class_=\"content\").text.strip()\n",
    "        apply_link = job.find(\"a\", class_=\"apply\")[\"href\"]\n",
    "        \n",
    "        job_listings.append((title, company, location, description, apply_link))\n",
    "    \n",
    "    for job in job_listings:\n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO jobs (job_title, company_name, location, job_description, application_link)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(job_title, company_name, location) DO UPDATE\n",
    "        SET job_description = excluded.job_description, application_link = excluded.application_link\n",
    "        \"\"\", job)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Job data scraped and stored.\")\n",
    "\n",
    "# Export filtered job data to CSV\n",
    "def export_jobs_to_csv(filter_by, value):\n",
    "    conn = sqlite3.connect(\"jobs.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"SELECT * FROM jobs WHERE {filter_by} = ?\", (value,))\n",
    "    jobs = cursor.fetchall()\n",
    "    \n",
    "    with open(\"filtered_jobs.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Job Title\", \"Company Name\", \"Location\", \"Description\", \"Application Link\"])\n",
    "        writer.writerows(jobs)\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"Filtered job data saved to filtered_jobs.csv.\")\n",
    "\n",
    "# Task 3: Scrape Laptop Data\n",
    "def scrape_laptops():\n",
    "    url = \"https://www.demoblaze.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    laptops = []\n",
    "    items = soup.find_all(\"div\", class_=\"card-block\")\n",
    "    \n",
    "    for item in items:\n",
    "        name = item.find(\"h4\", class_=\"card-title\").text.strip()\n",
    "        price = item.find(\"h5\").text.strip()\n",
    "        description = item.find(\"p\", class_=\"card-text\").text.strip()\n",
    "        laptops.append({\"name\": name, \"price\": price, \"description\": description})\n",
    "    \n",
    "    with open(\"laptops.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(laptops, file, indent=4)\n",
    "    \n",
    "    print(\"Laptop data scraped and saved to laptops.json.\")\n",
    "\n",
    "# Run tasks\n",
    "scrape_weather()\n",
    "scrape_jobs()\n",
    "scrape_laptops()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
